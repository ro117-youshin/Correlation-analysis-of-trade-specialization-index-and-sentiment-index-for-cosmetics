{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"craw2.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1bEEEAfNzn0RgrfJJyvSqjMLFRo2iQ3QC","authorship_tag":"ABX9TyM93S128vnrct1NqA0FC2Y+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UrBzZhTmzq14"},"outputs":[],"source":["!pip install transformers\n","!pip install sacremoses"]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"id":"z3933jmMzxLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from transformers import *\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import sentencepiece as spm"],"metadata":{"id":"7-yDDes4zxN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"id":"BspdhFtEzxQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 네이버 영화 감성분석 데이터 다운로드\n","!git clone https://github.com/e9t/nsmc.git"],"metadata":{"id":"278iP5SVzxTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.listdir('nsmc')"],"metadata":{"id":"MNb-IlpzzxWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n","test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")"],"metadata":{"id":"Vy1ljaup0n4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train[50:70]"],"metadata":{"id":"03sJ4fBDU8sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n","\n","from transformers import PreTrainedTokenizer\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n","\n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n","\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n","\n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n","\n","SPIECE_UNDERLINE = u'▁'\n","\n","\n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","\n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n","\n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n","\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","\n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n","\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n","\n","    def get_vocab(self):\n","        return dict(self.token2idx, **self.added_tokens_encoder)\n","\n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n","\n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n","\n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n","\n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n","\n","        return outputs\n","\n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n","\n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n","\n","        return new_pieces\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n","\n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n","\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A KoBERT sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n","\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n","\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A KoBERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n","\n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n","\n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n","\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n","\n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","\n","        return out_vocab_model, out_vocab_txt"],"metadata":{"id":"DrfgsPB6U8vd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"],"metadata":{"id":"PGGGu-2wU8yD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"],"metadata":{"id":"AQN6Kz6NU81G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"],"metadata":{"id":"TX5YifD6U83x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.tokenize(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))"],"metadata":{"id":"fOtmfx73U86h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))"],"metadata":{"id":"0YVrZfmaU89G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\", max_length=64, pad_to_max_length=True))"],"metadata":{"id":"Y9K0aXFRU8_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 세그멘트 인풋\n","print([0]*64)"],"metadata":{"id":"_ZH5ybC6U9CT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 마스크 인풋\n","valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n","print(valid_num * [1] + (64 - valid_num) * [0])"],"metadata":{"id":"mYKtjveYU9FV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_data(data_df):\n","    global tokenizer\n","    \n","    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n","    \n","    tokens, masks, segments, targets = [], [], [], []\n","    \n","    for i in tqdm(range(len(data_df))):\n","        # token : 문장을 토큰화함\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], truncation=True, padding='max_length', max_length=SEQ_LEN)\n","       \n","        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n","        num_zeros = token.count(0)\n","        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n","        \n","        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n","        segment = [0]*SEQ_LEN\n","\n","        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n","        tokens.append(token)\n","        masks.append(mask)\n","        segments.append(segment)\n","        \n","        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n","        targets.append(data_df[LABEL_COLUMN][i])\n","\n","    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    targets = np.array(targets)\n","\n","    return [tokens, masks, segments], targets\n","\n","# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n","def load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n","    data_x, data_y = convert_data(data_df)\n","    return data_x, data_y\n","\n","SEQ_LEN = 64\n","BATCH_SIZE = 32\n","# 긍부정 문장을 포함하고 있는 칼럼\n","DATA_COLUMN = \"document\"\n","# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n","LABEL_COLUMN = \"label\"\n","\n","# train 데이터를 버트 인풋에 맞게 변환\n","train_x, train_y = load_data(train)"],"metadata":{"id":"L8p05F-MU9IM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 훈련 성능을 검증한 test 데이터를 버트 인풋에 맞게 변환\n","test_x, test_y = load_data(test)"],"metadata":{"id":"XnjOYV9rU9LK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n","# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n","token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n","mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n","segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n","# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n","bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"],"metadata":{"id":"q1b1Ku4XU9N6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_outputs"],"metadata":{"id":"E4T7aWB3U9Qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_outputs = bert_outputs[1]"],"metadata":{"id":"vCrUQ9htU9TQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Rectified Adam 옵티마이저 사용\n","!pip install tensorflow_addons\n","import tensorflow_addons as tfa\n","# 총 batch size * 4 epoch = 2344 * 4\n","opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"],"metadata":{"id":"heBilRVuU9WK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n","sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n","sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n","sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"],"metadata":{"id":"QN27ScYVU9Yc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_model.summary()"],"metadata":{"id":"2fVl5LJRU9bN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_model.fit(train_x, train_y, epochs=2, shuffle=True, batch_size=64, validation_data=(test_x, test_y))"],"metadata":{"id":"WA7JmK900X5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_convert_data(data_df):\n","    global tokenizer\n","    tokens, masks, segments = [], [], []\n","    \n","    for i in tqdm(range(len(data_df))):\n","\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n","        num_zeros = token.count(0)\n","        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n","        segment = [0]*SEQ_LEN\n","\n","        tokens.append(token)\n","        segments.append(segment)\n","        masks.append(mask)\n","\n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    return [tokens, masks, segments]\n","\n","# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n","def predict_load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_x = predict_convert_data(data_df)\n","    return data_x"],"metadata":{"id":"8ma8nYRgVjTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_set = predict_load_data(test)"],"metadata":{"id":"ywP877QRVjVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_set"],"metadata":{"id":"PadZO6FjVjYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = sentiment_model.predict(test_set)"],"metadata":{"id":"9r41Ud7VVjb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 부정이면 0, 긍정이면 1 출력\n","preds"],"metadata":{"id":"mFOQZLxuVje5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","y_true = test['label']\n","# F1 Score 확인\n","print(classification_report(y_true, np.round(preds,0)))"],"metadata":{"id":"Zh6ccDILVjh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","tf.get_logger().setLevel(logging.ERROR)"],"metadata":{"id":"W62YnihdVjko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sentence_convert_data(data):\n","    global tokenizer\n","    tokens, masks, segments = [], [], []\n","    token = tokenizer.encode(data, max_length=SEQ_LEN, truncation=True, padding='max_length')\n","    \n","    num_zeros = token.count(0) \n","    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros \n","    segment = [0]*SEQ_LEN\n","\n","    tokens.append(token)\n","    segments.append(segment)\n","    masks.append(mask)\n","\n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    return [tokens, masks, segments]\n","\n","def movie_evaluation_predict(sentence):\n","    data_x = sentence_convert_data(sentence)\n","    predict = sentiment_model.predict(data_x)\n","    predict_value = np.ravel(predict)\n","    predict_answer = np.round(predict_value,0).item()\n","    \n","    if predict_answer == 0:\n","        #긍정지수\n","        return float((1-predict_value)*-1)\n","    elif predict_answer == 1:\n","        #부정지수\n","        return float(predict_value)"],"metadata":{"id":"YkpPH9ZmVjnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"%.2f\"% movie_evaluation_predict(\"보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에 \"))"],"metadata":{"id":"JcfW4FUvVjqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["typeToFloat = float(movie_evaluation_predict(\"프로젝트 잘 됬으면 좋겠다!\"))\n","print(typeToFloat)"],"metadata":{"id":"jrtLC6VBokRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_evaluation_predict(\"대리기사와 다투고 “사고 피하려고” 400m 음주운전…항소심도 무죄\")"],"metadata":{"id":"i1DruH60or8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_evaluation_predict(\"대구시, 뷰티산업 육성·수출 활성화 지원 총력\")"],"metadata":{"id":"xtnxUOSXsNSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_evaluation_predict(\"[발자취] “기업은 나라와 국민 부강하게 해야” 국산 화장품 해외 수출한 산업화 주역\")"],"metadata":{"id":"3Q0XZE1UsVsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_evaluation_predict(\"[제약+] HK이노엔, 인도 등 7개국 '케이캡' 수출 계약 체결 外\")"],"metadata":{"id":"oL42tubMill_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_evaluation_predict(\"기사제목\")"],"metadata":{"id":"UAp7HLc5i6Bi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_evaluation_predict(\"윤석열 문재인도 있는데 외계인이 없을리가 ㅋㅋㅋ \")\n","movie_evaluation_predict(\"아스트라제네카\")\n","movie_evaluation_predict(\"외계인이라기 보단 뭔가 있긴'있나봄.\")\n","movie_evaluation_predict(\"재미있나 봐?\")\n","movie_evaluation_predict(\"있다고 치자 지구까지 왔는데 관광만 하고 갔을때 지구까지 올 정도면 인간이 상상하기도 어려울 정도로 문명과 과학이 발달 했을텐데 그냥 돌아가기엔 아쉽지 않을까 지구의 어느 나라든 물리적 흔적을 남겼을 것 같은데 그리고 미국이나 소련놈들이 한가하게 냉전시대를 계속 이어가겠냐고 월등히 앞서있는 외계 생명체하고 대적을 하지\")\n","movie_evaluation_predict(\"삼각형은 '아스트라'라고 미국이 만든거임. \")\n","movie_evaluation_predict(\"이 빌어드실 지구에 뭐 먹을거 있다고 오셨어여~~\")\n","movie_evaluation_predict(\"인간 대단한 지식적 존재가 아니라 개인적 욕심을보고사는 3차원의 존재들일뿐이다. 없으면 탐하고 남에게 잘보이고싶은 심리가 더 강하다. 정말 부끄러운 존재다. 만약 정말고차원적인 존재 하나가 지구상에 나타나 우리가 생각하고 판단하는 것에 넘어설수없는 지식과 지성을 가지고있다면 그들은 우리처럼 아는척하고 보여주려하고 뺐으려느른 행동이 아무 의미가 없다고 느낄수있다. 우리와는 차원이다른 생각의 소유자들이 존재한다는 말이다. 인간의 몸을 이루고있는 원자를 축소하면 180센치미터의 사람이라도 각설탕만하다고 과학자들이 이야기한다. 우리는없다\")\n","movie_evaluation_predict(\"미군은 외계인하거 거래한다더니..뜬금? \")\n","movie_evaluation_predict(\"빵상깨랑 까랑즈ㅡㅠ 빠라랑 트튜 파랑츠ㅠㅠ파랑르ㅡ 여기 휴게소 졸음 운전 주의 \")\n","movie_evaluation_predict(\"외계에서 온 게 아니라고 하면 또 그..... 뭐냐 지구 안에 또 다른 지구 있다는 음모론 ㅋㅋㅋㅋ 지구 공동설인가? 그 얘기 좋아하는 애들 튀어나올 듯 \")\n","movie_evaluation_predict(\"빨리사진지워!!!!기억을 없앨지몰라\")\n","movie_evaluation_predict(\"어디서 이빨을?!\")\n","movie_evaluation_predict(\"내주변에도 외계인많던데 ! \")\n","movie_evaluation_predict(\"지구에 왔든 안왔든 반듯이 외계인은 있다. 우리와같은 태양계가 셀수없이 무한하게 존재하니까... \")\n","movie_evaluation_predict(\"보고하는 조종사를 낙인 찍는 관행은 또 뭐야미국도 웃기는 나라군.본 걸 봤다고 하는데..\")\n","movie_evaluation_predict(\"도르마무와 캡틴마블이 지구 째리고 있다 조심해라... \")\n"],"metadata":{"id":"YJAGuLNvzIxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","df1 = pd.read_csv(\"/content/gdrive/MyDrive/csv/k뷰티_수출.csv\",  encoding=\"UTF-8\")\n","df1.columns = ['index', 'date', 'script', 'url']\n","df1['datetime']=df1['date'].apply(lambda x: pd.to_datetime(str(x), format=\"%Y.%m.%d.\"))\n","df1.set_index('datetime', inplace=True)\n","df1\n","#movie_evaluation_predict(str(df1.loc[:,\"기사제목\"]))"],"metadata":{"id":"KXqqIJ1Yg5pU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1['year']=df1.index.year\n","df1['month']=df1.index.month \n","df1['day']=df1.index.day\n","df1"],"metadata":{"id":"91boPzVPoDqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat = df1['script']\n","cat"],"metadata":{"id":"Ndet3wP4oWv5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat_val=cat.values\n","cat_val"],"metadata":{"id":"s1CwE8_ApkTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat_list=cat_val.tolist()\n","cat_list\n","#cat.values.tolist() 도 가능"],"metadata":{"id":"iZlSqn78py59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["value_rate = []\n","sum = 0.0\n","num = 0\n","for i in cat_list:\n","    value_rate.append(\"%.2f\"% (movie_evaluation_predict(i)))\n","    num+=1\n","value_rate"],"metadata":{"id":"bG1rbpRSqMhz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["value_list=pd.DataFrame(value_rate)\n","value_list.to_csv(\"/content/gdrive/MyDrive/csv/rate.csv\", encoding='utf-8')"],"metadata":{"id":"qwDFN0RufVSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rate = pd.read_csv(\"/content/gdrive/MyDrive/csv/rate.csv\", encoding=\"UTF-8\")\n","rate.columns = ['index','rate']\n","rate"],"metadata":{"id":"1oGtmZ0gpR7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a=pd.merge(df1, rate)\n","a"],"metadata":{"id":"L6E20M4RsWVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.groupby(['year', 'month'])['rate'].mean().round(2)"],"metadata":{"id":"SpGEOYplshgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_data = a.groupby(['year', 'month'])['rate'].mean().round(2).reset_index()\n","sorted_data"],"metadata":{"id":"Da_GrIa1sidb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2=pd.DataFrame(sorted_data)\n","df2.to_csv(\"/content/gdrive/MyDrive/csv/rateMean.csv\", encoding='utf-8')"],"metadata":{"id":"UWg6yMu1skLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_data.plot(x=('month'), y='rate')"],"metadata":{"id":"LtSkIW9U4uGx"},"execution_count":null,"outputs":[]}]}